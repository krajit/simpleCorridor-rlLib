{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb1c6ce-26be-46df-b1ae-e255022e63ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59fdce-9ab7-4e89-845f-0f7a755ba538",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tutorial\n",
    "https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a61fced-05bb-4893-8931-8a8feec57472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "envName = \"MountainCar-v0\" # \"CartPole-v0\"\n",
    "env = gym.make(envName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2047b407-a633-46b8-9f7d-b4367bc3825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cfc8c08-ec20-4b33-8b57-53051bd7b3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b230a60f-0760-4379-8da1-93220ef0083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55466765,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26db5e4-1522-4a46-83f2-9499aecef465",
   "metadata": {},
   "outputs": [
    {
     "ename": "GLException",
     "evalue": "b'invalid operation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGLException\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mcla()\n\u001b[1;32m      8\u001b[0m state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n\u001b[0;32m---> 10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     12\u001b[0m i \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/envs/classic_control/mountain_car.py:168\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcartrans\u001b[38;5;241m.\u001b[39mset_translation(\n\u001b[1;32m    164\u001b[0m     (pos \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_position) \u001b[38;5;241m*\u001b[39m scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_height(pos) \u001b[38;5;241m*\u001b[39m scale\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcartrans\u001b[38;5;241m.\u001b[39mset_rotation(math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m pos))\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:135\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_rgb_array:\n\u001b[1;32m    134\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m pyglet\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mget_buffer_manager()\u001b[38;5;241m.\u001b[39mget_color_buffer()\n\u001b[0;32m--> 135\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(image_data\u001b[38;5;241m.\u001b[39mget_data(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# In https://github.com/openai/gym-http-api/issues/2, we\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# discovered that someone using Xmonad on Arch was having\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# a window of size 598 x 398, though a 600 x 400 window\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# was requested. (Guess Xmonad was preserving a pixel for\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# the boundary.) So we use the buffer height/width rather\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# than the requested one.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/pyglet/image/__init__.py:2049\u001b[0m, in \u001b[0;36mBufferImage.get_image_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2047\u001b[0m glPushClientAttrib(GL_CLIENT_PIXEL_STORE_BIT)\n\u001b[1;32m   2048\u001b[0m glPixelStorei(GL_PACK_ALIGNMENT, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2049\u001b[0m \u001b[43mglReadPixels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgl_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGL_UNSIGNED_BYTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m glPopClientAttrib()\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ImageData(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat, buffer)\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/pyglet/gl/lib.py:107\u001b[0m, in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mcast(gl\u001b[38;5;241m.\u001b[39mgluErrorString(error), ctypes\u001b[38;5;241m.\u001b[39mc_char_p)\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GLException(msg)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mGLException\u001b[0m: b'invalid operation'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    plt.cla()\n",
    "    \n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    time.sleep(0.01)\n",
    "    i = i+ 1\n",
    "    print(i, reward)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.gcf()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3b354-6b8b-4864-bf4c-56f83b7ff442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "config = {\n",
    "    \"env\": envName,\n",
    "    # Change the following line to `“framework”: “tf”` to use tensorflow\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "      \"fcnet_hiddens\": [32],\n",
    "      \"fcnet_activation\": \"linear\",\n",
    "    },\n",
    "}\n",
    "stop = {\"episode_reward_mean\": -0.05}\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_cpus=3,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "# execute training \n",
    "analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=config,\n",
    "  stop=stop,\n",
    "  checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dff4153c-229f-4cec-b4f4-7de589e32448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 12:55:54,713\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit.kumar@SNU.IN/ray_results/PPO/PPO_CartPole-v0_6363a_00000_0_2022-06-29_12-53-35/checkpoint_000019/checkpoint-19\n",
      "2022-06-29 12:55:54,714\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 19, '_timesteps_total': None, '_time_total': 58.98848032951355, '_episodes_total': 845}\n"
     ]
    }
   ],
   "source": [
    "# restore a trainer from the last checkpoint\n",
    "trial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "  trial,\n",
    "  \"training_iteration\",\n",
    "  \"max\",\n",
    ")\n",
    "trainer = PPOTrainer(config=config)\n",
    "trainer.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6408814c-cd13-4ba0-ab67-2884741e66a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mcla()\n\u001b[0;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_action(state)\n\u001b[1;32m      9\u001b[0m     state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     10\u001b[0m     i \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    plt.cla()\n",
    "\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    i = i+1\n",
    "    print(i)\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.gcf()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f3f1c-c95c-48e9-a65c-0d0c8b7383b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "config = {\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    # Change the following line to `“framework”: “tf”` to use tensorflow\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "      \"fcnet_hiddens\": [32],\n",
    "      \"fcnet_activation\": \"linear\",\n",
    "    },\n",
    "}\n",
    "stop = {\"episode_reward_mean\": 195}\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_cpus=3,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "# execute training \n",
    "analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=config,\n",
    "  stop=stop,\n",
    "  checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c56e219-8078-4e8e-ae5c-c0cc73ae526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 10:58:47,456\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-28 10:58:47,457\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-06-28 10:58:48,870\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit.kumar@SNU.IN/ray_results/PPO/PPO_CartPole-v0_e1b6d_00000_0_2022-06-28_10-56-37/checkpoint_000034/checkpoint-34\n",
      "2022-06-28 10:58:48,871\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 34, '_timesteps_total': None, '_time_total': 104.47818350791931, '_episodes_total': 1159}\n"
     ]
    }
   ],
   "source": [
    "# restore a trainer from the last checkpoint\n",
    "trial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "  trial,\n",
    "  \"training_iteration\",\n",
    "  \"max\",\n",
    ")\n",
    "trainer = PPOTrainer(config=config)\n",
    "trainer.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584158e8-dfc1-43ef-af46-db1dc6d10f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "state = env.reset()\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    plt.cla()\n",
    "\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    i = i+1\n",
    "    print(i)\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.gcf()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c3fd22c-93ad-471b-ab5f-ab17ea07c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507eb0a-06c3-46ef-8c85-27f2d5cdc9c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ray tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a971168-0f62-4492-8c6e-440b1f89d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search_config = {\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    \"framework\": \"torch\",\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    \"model\": {\n",
    "      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n",
    "      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n",
    "    },\n",
    "    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n",
    "}\n",
    "\n",
    "# To explicitly stop or restart Ray, use the shutdown API.\n",
    "ray.shutdown()\n",
    "\n",
    "ray.init(\n",
    "  num_cpus=12,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")\n",
    "\n",
    "parameter_search_analysis = ray.tune.run(\n",
    "  \"PPO\",\n",
    "  config=parameter_search_config,\n",
    "  stop=stop,\n",
    "  num_samples=5,\n",
    "  metric=\"timesteps_total\",\n",
    "  mode=\"min\",\n",
    ")\n",
    "\n",
    "print(\n",
    "  \"Best hyperparameters found:\",\n",
    "  parameter_search_analysis.best_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc619e7-4734-46fb-acb8-41cb008f87e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'env': 'CartPole-v0', 'framework': 'torch', 'model': {'fcnet_hiddens': [32], 'fcnet_activation': 'relu'}, 'lr': 0.002069047627722433}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  \"Best hyperparameters found:\",\n",
    "  parameter_search_analysis.best_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bb8f7-7161-4ce6-bdbb-a4c7d853555e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Env: Simple Corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b20ab14-b700-49e3-95a4-00236b741794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "\n",
    "# Define your problem using python and openAI's gym API:\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n",
    "\n",
    "    ---------------------\n",
    "    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n",
    "    ---------------------\n",
    "\n",
    "    Possible actions to chose from are: 0=left; 1=right\n",
    "    Observations are floats indicating the current field index, e.g. 0.0 for\n",
    "    starting position, 1.0 for the field next to the starting position, etc..\n",
    "    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = gym.spaces.Discrete(2)  # left and right\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\"\"\"\n",
    "        self.cur_pos = 0\n",
    "        # Return initial observation.\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Walk left.\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right.\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        # Set `done` flag when end of corridor (goal) reached.\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # +1 when goal reached, otherwise -1.\n",
    "        reward = 1.0 if done else -0.1\n",
    "        return [self.cur_pos], reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e88a37-dd14-41d7-a952-8a89723cd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleCorridor({\"corridor_length\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f888d0-5767-45a0-9124-f291d03b3503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008da0ba-3082-44c2-bd71-a77de215a0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abe936-f66b-45e4-9720-7180775d9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RLlib Trainer instance.\n",
    "trainer = PPOTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": SimpleCorridor,\n",
    "        \"framework\": \"torch\",\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "            # Use corridor with 20 fields (including S and G).\n",
    "            \"corridor_length\": 20\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 8,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to move at least 19 times in the env to reach the goal and\n",
    "# each move gives us -0.1 reward (except the last move at the end: +1.0),\n",
    "# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\n",
    "for i in range(5):\n",
    "    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99181e4c-c2ff-4d5c-b12c-e75af694bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf71ffae-eb02-4837-b9d8-3f226458c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 1 episode; total-reward=-0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly different env here (len 10 instead of 20),\n",
    "# however, this should still work as the agent has (hopefully) learned\n",
    "# to \"just always walk right!\"\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = trainer.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "# Report results.\n",
    "print(f\"Played 1 episode; total-reward={total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132b147f-20fd-4ecc-a34e-0a8cd3da62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa86bab-078a-407c-a6bf-bef892324041",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b7c52-9a0f-4152-8e4a-982b556e8c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
